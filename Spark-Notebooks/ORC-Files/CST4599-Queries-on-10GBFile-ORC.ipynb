{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries and initializing Spark context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/usr/local/spark')\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.config(\"spark.executor.memory\",\"25g\").config(\"spark.driver.memory\",\"25g\").config(\"spark.memory.offHeap.enabled\",\"true\").config(\"spark.memory.offHeap.size\",\"32g\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading .csv files into individual dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "filePath_10gb = \"../CSV-Files/nasa_logs_10GB.csv\"\n",
    "df_10gb = spark.read.format('csv').option(\"header\",\"false\").option(\"inferSchema\",\"true\").load(filePath_10gb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displaying total number of loaded records in each dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_10gb.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Renaming column names into meaningful names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_10gb = df_10gb.withColumnRenamed(\"_c0\",\"host\") \\\n",
    "                .withColumnRenamed(\"_c1\",\"method\") \\\n",
    "                .withColumnRenamed(\"_c2\",\"endpoint\") \\\n",
    "                .withColumnRenamed(\"_c3\",\"protocol\") \\\n",
    "                .withColumnRenamed(\"_c4\",\"status\") \\\n",
    "                .withColumnRenamed(\"_c5\",\"object_size\") \\\n",
    "                .withColumnRenamed(\"_c6\",\"timestamp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting dataframe into ORC file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_10gb.write.orc(\"nasa_logs_10GB.orc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading ORC file into dataframe to be able to query it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "orcPath_10gb = spark.read.orc(\"./nasa_logs_10GB.orc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a view from dataframe to a meaningful name that can be used in the queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orcPath_10gb.createOrReplaceTempView(\"http_logs_orc_10gb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query 1: Count the number of records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "query1_10gb = spark.sql(\"select count(*) AS TOTAL_RECORDS from http_logs_orc_10gb\")\n",
    "query1_10gb.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query 2: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "query2_10gb = spark.sql(\"SELECT endpoint, COUNT(*) AS page_view_count FROM http_logs_orc_10gb \\\n",
    "                        GROUP BY endpoint \\\n",
    "                        ORDER BY page_view_count DESC LIMIT 5\")\n",
    "query2_10gb.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "query3_10gb = spark.sql(\"SELECT status, count(status) AS distinct_status FROM http_logs_orc_10gb \\\n",
    "                        WHERE status >= '400' \\\n",
    "                        GROUP BY status \\\n",
    "                        ORDER BY distinct_status DESC\")\n",
    "query3_10gb.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "query4_10gb = spark.sql(\"SELECT endpoint, count(endpoint) AS count_of_requests \\\n",
    "                        FROM http_logs_orc_10gb WHERE status >= '400' \\\n",
    "                        GROUP BY endpoint \\\n",
    "                        ORDER BY count_of_requests DESC \\\n",
    "                        LIMIT 5\")\n",
    "query4_10gb.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "query5_10gb = spark.sql(\"SELECT DISTINCT(endpoint), timestamp, ROUND((object_size * 0.000001)) AS SIZE_IN_MB \\\n",
    "                        FROM http_logs_orc_10gb \\\n",
    "                        ORDER BY SIZE_IN_MB DESC \\\n",
    "                        LIMIT 20\")\n",
    "query5_10gb.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
