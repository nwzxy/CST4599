{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries and initializing Spark context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/usr/local/spark')\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.config(\"spark.executor.memory\",\"25g\").config(\"spark.driver.memory\",\"25g\").config(\"spark.memory.offHeap.enabled\",\"true\").config(\"spark.memory.offHeap.size\",\"32g\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading .csv files into individual dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.65 ms, sys: 541 µs, total: 5.2 ms\n",
      "Wall time: 15.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "filePath_1gb = \"./CSV-Files/nasa_logs_1GB.csv\"\n",
    "df_1gb = spark.read.format('csv').option(\"header\",\"false\").option(\"inferSchema\",\"true\").load(filePath_1gb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displaying total number of loaded records in each dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.69 ms, sys: 195 µs, total: 1.89 ms\n",
      "Wall time: 2.26 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "13846448"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "df_1gb.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Renaming column names into meaningful names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1gb = df_1gb.withColumnRenamed(\"_c0\",\"host\") \\\n",
    "                .withColumnRenamed(\"_c1\",\"method\") \\\n",
    "                .withColumnRenamed(\"_c2\",\"endpoint\") \\\n",
    "                .withColumnRenamed(\"_c3\",\"protocol\") \\\n",
    "                .withColumnRenamed(\"_c4\",\"status\") \\\n",
    "                .withColumnRenamed(\"_c5\",\"object_size\") \\\n",
    "                .withColumnRenamed(\"_c6\",\"timestamp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting dataframes into Parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_1gb.write.parquet(\"nasa_logs_1GB.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Parquet files into dataframes to be able to query them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 ms, sys: 238 µs, total: 2.24 ms\n",
      "Wall time: 1.18 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "prqPath_1gb = spark.read.parquet(\"./Parquet-Files/nasa_logs_1GB.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a view from dataframes to a meaningful name that can be used in the queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prqPath_1gb.createOrReplaceTempView(\"http_logs_prq_1gb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query 1: Count the number of records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|TOTAL_RECORDS|\n",
      "+-------------+\n",
      "|     13846448|\n",
      "+-------------+\n",
      "\n",
      "CPU times: user 1.74 ms, sys: 0 ns, total: 1.74 ms\n",
      "Wall time: 890 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "query1_1gb = spark.sql(\"select count(*) AS TOTAL_RECORDS from http_logs_prq_1gb\")\n",
    "query1_1gb.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query 2: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+\n",
      "|            endpoint|page_view_count|\n",
      "+--------------------+---------------+\n",
      "|/images/NASA-logo...|         834856|\n",
      "|/images/KSC-logos...|         659880|\n",
      "|/images/MOSAIC-lo...|         511632|\n",
      "|/images/USA-logos...|         508296|\n",
      "|/images/WORLD-log...|         503700|\n",
      "+--------------------+---------------+\n",
      "\n",
      "CPU times: user 0 ns, sys: 3.09 ms, total: 3.09 ms\n",
      "Wall time: 3.34 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "query2_1gb = spark.sql(\"SELECT endpoint, COUNT(*) AS page_view_count FROM http_logs_prq_1gb \\\n",
    "                        GROUP BY endpoint \\\n",
    "                        ORDER BY page_view_count DESC LIMIT 5\")\n",
    "query2_1gb.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------+\n",
      "|status|distinct_status|\n",
      "+------+---------------+\n",
      "|   404|          83596|\n",
      "|   403|            900|\n",
      "|   500|            260|\n",
      "|   501|            164|\n",
      "|   400|             60|\n",
      "+------+---------------+\n",
      "\n",
      "CPU times: user 2.69 ms, sys: 0 ns, total: 2.69 ms\n",
      "Wall time: 1.23 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "query3_1gb = spark.sql(\"SELECT status, count(status) AS distinct_status FROM http_logs_prq_1gb \\\n",
    "                        WHERE status >= '400' \\\n",
    "                        GROUP BY status \\\n",
    "                        ORDER BY distinct_status DESC\")\n",
    "query3_1gb.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------+\n",
      "|            endpoint|count_of_requests|\n",
      "+--------------------+-----------------+\n",
      "|/pub/winvn/readme...|             8016|\n",
      "|/pub/winvn/releas...|             6928|\n",
      "|/shuttle/missions...|             2732|\n",
      "|/shuttle/missions...|             1712|\n",
      "|/history/apollo/a...|             1536|\n",
      "+--------------------+-----------------+\n",
      "\n",
      "CPU times: user 2.58 ms, sys: 0 ns, total: 2.58 ms\n",
      "Wall time: 1.45 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "query4_1gb = spark.sql(\"SELECT endpoint, count(endpoint) AS count_of_requests \\\n",
    "                        FROM http_logs_prq_1gb WHERE status >= '400' \\\n",
    "                        GROUP BY endpoint \\\n",
    "                        ORDER BY count_of_requests DESC \\\n",
    "                        LIMIT 5\")\n",
    "query4_1gb.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+----------+\n",
      "|            endpoint|          timestamp|SIZE_IN_MB|\n",
      "+--------------------+-------------------+----------+\n",
      "|/shuttle/countdow...|1995-07-07 14:03:32|         7|\n",
      "|/statistics/1995/...| 1995-07-09 9:18:44|         3|\n",
      "|/statistics/1995/...|1995-08-11 11:16:21|         3|\n",
      "|/statistics/1995/...|1995-08-07 18:28:57|         3|\n",
      "|/statistics/1995/...|1995-08-21 14:21:16|         3|\n",
      "|/mdss/ped/acs/SDP.ps|1995-07-11 17:29:34|         3|\n",
      "|/statistics/1995/...|1995-08-03 15:51:23|         3|\n",
      "|/statistics/1995/...|1995-08-21 10:54:33|         3|\n",
      "|/statistics/1995/...|1995-07-07 10:28:56|         3|\n",
      "|/statistics/1995/...|1995-07-06 10:19:00|         3|\n",
      "|/statistics/1995/...| 1995-07-28 9:13:09|         3|\n",
      "|/statistics/1995/...| 1995-07-05 8:57:07|         3|\n",
      "|/statistics/1995/...|1995-08-17 10:13:42|         3|\n",
      "|/statistics/1995/...|1995-07-06 12:11:57|         3|\n",
      "|/statistics/1995/...|1995-07-05 17:21:54|         3|\n",
      "|/statistics/1995/...| 1995-08-21 8:43:56|         3|\n",
      "|/statistics/1995/...|1995-08-11 13:35:36|         3|\n",
      "|/statistics/1995/...|1995-07-03 22:02:47|         3|\n",
      "|/statistics/1995/...|1995-07-04 17:20:20|         3|\n",
      "|/statistics/1995/...| 1995-07-09 9:22:14|         3|\n",
      "+--------------------+-------------------+----------+\n",
      "\n",
      "CPU times: user 4.88 ms, sys: 75 µs, total: 4.96 ms\n",
      "Wall time: 11.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "query5_1gb = spark.sql(\"SELECT DISTINCT(endpoint), timestamp, ROUND((object_size * 0.000001)) AS SIZE_IN_MB \\\n",
    "                        FROM http_logs_prq_1gb \\\n",
    "                        ORDER BY SIZE_IN_MB DESC \\\n",
    "                        LIMIT 20\")\n",
    "query5_1gb.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
