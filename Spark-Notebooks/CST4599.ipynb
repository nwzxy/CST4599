{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries and initializing Spark context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/usr/local/spark')\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.config(\"spark.executor.memory\",\"25g\").config(\"spark.driver.memory\",\"25g\").config(\"spark.memory.offHeap.enabled\",\"true\").config(\"spark.memory.offHeap.size\",\"32g\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading .csv files into individual dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filePath_1gb = \"./CSV-Files/nasa_logs_1GB.csv\"\n",
    "df_1gb = spark.read.format('csv').option(\"header\",\"false\").option(\"inferSchema\",\"true\").load(filePath_1gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "filePath_5gb = \"./CSV-Files/nasa_logs_5GB.csv\"\n",
    "df_5gb = spark.read.format('csv').option(\"header\",\"false\").option(\"inferSchema\",\"true\").load(filePath_5gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "filePath_10gb = \"./CSV-Files/nasa_logs_10GB.csv\"\n",
    "df_10gb = spark.read.format('csv').option(\"header\",\"false\").option(\"inferSchema\",\"true\").load(filePath_10gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.3 ms, sys: 6.39 ms, total: 21.7 ms\n",
      "Wall time: 2min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "filePath_15gb = \"./CSV-Files/nasa_logs_15GB.csv\"\n",
    "df_15gb = spark.read.format('csv').option(\"header\",\"false\").option(\"inferSchema\",\"true\").load(filePath_15gb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displaying total number of loaded records in each dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13846448"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1gb.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58847404"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_5gb.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114233196"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_10gb.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "173080600"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_15gb.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Renaming column names into meaningful names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1gb = df_1gb.withColumnRenamed(\"_c0\",\"host\") \\\n",
    "                .withColumnRenamed(\"_c1\",\"method\") \\\n",
    "                .withColumnRenamed(\"_c2\",\"endpoint\") \\\n",
    "                .withColumnRenamed(\"_c3\",\"protocol\") \\\n",
    "                .withColumnRenamed(\"_c4\",\"status\") \\\n",
    "                .withColumnRenamed(\"_c5\",\"object_size\") \\\n",
    "                .withColumnRenamed(\"_c6\",\"timestamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_5gb = df_5gb.withColumnRenamed(\"_c0\",\"host\") \\\n",
    "                .withColumnRenamed(\"_c1\",\"method\") \\\n",
    "                .withColumnRenamed(\"_c2\",\"endpoint\") \\\n",
    "                .withColumnRenamed(\"_c3\",\"protocol\") \\\n",
    "                .withColumnRenamed(\"_c4\",\"status\") \\\n",
    "                .withColumnRenamed(\"_c5\",\"object_size\") \\\n",
    "                .withColumnRenamed(\"_c6\",\"timestamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_10gb = df_10gb.withColumnRenamed(\"_c0\",\"host\") \\\n",
    "                .withColumnRenamed(\"_c1\",\"method\") \\\n",
    "                .withColumnRenamed(\"_c2\",\"endpoint\") \\\n",
    "                .withColumnRenamed(\"_c3\",\"protocol\") \\\n",
    "                .withColumnRenamed(\"_c4\",\"status\") \\\n",
    "                .withColumnRenamed(\"_c5\",\"object_size\") \\\n",
    "                .withColumnRenamed(\"_c6\",\"timestamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_15gb = df_15gb.withColumnRenamed(\"_c0\",\"host\") \\\n",
    "                .withColumnRenamed(\"_c1\",\"method\") \\\n",
    "                .withColumnRenamed(\"_c2\",\"endpoint\") \\\n",
    "                .withColumnRenamed(\"_c3\",\"protocol\") \\\n",
    "                .withColumnRenamed(\"_c4\",\"status\") \\\n",
    "                .withColumnRenamed(\"_c5\",\"object_size\") \\\n",
    "                .withColumnRenamed(\"_c6\",\"timestamp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting dataframes into Parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "path file:/home/ubuntu/dataset/nasa_logs_1GB.parquet already exists.;",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-04a26d4ff2b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_1gb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nasa_logs_1GB.parquet\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m    934\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_opts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 936\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    937\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    938\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: path file:/home/ubuntu/dataset/nasa_logs_1GB.parquet already exists.;"
     ]
    }
   ],
   "source": [
    "# df_1gb.write.parquet(\"./Parquet-Files/nasa_logs_1GB.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_5gb.write.parquet(\"nasa_logs_5GB.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_10gb.write.parquet(\"nasa_logs_10GB.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_15gb.write.parquet(\"nasa_logs_15GB.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Parquet files into dataframes to be able to query them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "prqPath_1gb = spark.read.parquet(\"nasa_logs_1GB.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "prqPath_5gb = spark.read.parquet(\"nasa_logs_5GB.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "prqPath_10gb = spark.read.parquet(\"nasa_logs_10GB.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "prqPath_15gb = spark.read.parquet(\"nasa_logs_15GB.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a view from dataframes to a meaningful name that can be used in the queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "prqPath_1gb.createOrReplaceTempView(\"http_logs_prq_1gb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "prqPath_5gb.createOrReplaceTempView(\"http_logs_prq_5gb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "prqPath_10gb.createOrReplaceTempView(\"http_logs_prq_10gb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "prqPath_15gb.createOrReplaceTempView(\"http_logs_prq_15gb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query 1: Count the number of records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.46 ms, sys: 0 ns, total: 1.46 ms\n",
      "Wall time: 29.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "query1_1gb = spark.sql(\"select count(*) from http_logs_prq_1gb\")\n",
    "query1_1gb.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.18 ms, sys: 200 µs, total: 1.38 ms\n",
      "Wall time: 6.24 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "query1_5gb = spark.sql(\"select count(*) from http_logs_prq_5gb\")\n",
    "query1_5gb.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.17 ms, sys: 198 µs, total: 1.37 ms\n",
      "Wall time: 5.57 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "query1_10gb = spark.sql(\"select count(*) from http_logs_prq_10gb\")\n",
    "query1_10gb.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.3 ms, sys: 217 µs, total: 1.51 ms\n",
      "Wall time: 5.46 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "query1_15gb = spark.sql(\"select count(*) from http_logs_prq_15gb\")\n",
    "query1_15gb.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query 2: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+\n",
      "|            endpoint|page_view_count|\n",
      "+--------------------+---------------+\n",
      "|/images/NASA-logo...|         834856|\n",
      "|/images/KSC-logos...|         659880|\n",
      "|/images/MOSAIC-lo...|         511632|\n",
      "|/images/USA-logos...|         508296|\n",
      "|/images/WORLD-log...|         503700|\n",
      "+--------------------+---------------+\n",
      "\n",
      "CPU times: user 1.95 ms, sys: 283 µs, total: 2.24 ms\n",
      "Wall time: 989 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "query2_1gb = spark.sql(\"SELECT endpoint, COUNT(*) AS page_view_count FROM http_logs_prq_1gb \\\n",
    "                        GROUP BY endpoint \\\n",
    "                        ORDER BY page_view_count DESC LIMIT 5\")\n",
    "query2_1gb.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+\n",
      "|            endpoint|page_view_count|\n",
      "+--------------------+---------------+\n",
      "|/images/NASA-logo...|        3548138|\n",
      "|/images/KSC-logos...|        2804490|\n",
      "|/images/MOSAIC-lo...|        2174436|\n",
      "|/images/USA-logos...|        2160258|\n",
      "|/images/WORLD-log...|        2140725|\n",
      "+--------------------+---------------+\n",
      "\n",
      "CPU times: user 2.5 ms, sys: 0 ns, total: 2.5 ms\n",
      "Wall time: 2.17 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "query2_5gb = spark.sql(\"SELECT endpoint, COUNT(*) AS page_view_count FROM http_logs_prq_5gb \\\n",
    "                        GROUP BY endpoint \\\n",
    "                        ORDER BY page_view_count DESC LIMIT 5\")\n",
    "query2_5gb.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+\n",
      "|            endpoint|page_view_count|\n",
      "+--------------------+---------------+\n",
      "|/images/NASA-logo...|        6887562|\n",
      "|/images/KSC-logos...|        5444010|\n",
      "|/images/MOSAIC-lo...|        4220964|\n",
      "|/images/USA-logos...|        4193442|\n",
      "|/images/WORLD-log...|        4155525|\n",
      "+--------------------+---------------+\n",
      "\n",
      "CPU times: user 2.73 ms, sys: 399 µs, total: 3.13 ms\n",
      "Wall time: 4.48 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "query2_10gb = spark.sql(\"SELECT endpoint, COUNT(*) AS page_view_count FROM http_logs_prq_10gb \\\n",
    "                        GROUP BY endpoint \\\n",
    "                        ORDER BY page_view_count DESC LIMIT 5\")\n",
    "query2_10gb.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+\n",
      "|            endpoint|page_view_count|\n",
      "+--------------------+---------------+\n",
      "|/images/NASA-logo...|       10435700|\n",
      "|/images/KSC-logos...|        8248500|\n",
      "|/images/MOSAIC-lo...|        6395400|\n",
      "|/images/USA-logos...|        6353700|\n",
      "|/images/WORLD-log...|        6296250|\n",
      "+--------------------+---------------+\n",
      "\n",
      "CPU times: user 2.92 ms, sys: 434 µs, total: 3.36 ms\n",
      "Wall time: 6.61 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "query2_15gb = spark.sql(\"SELECT endpoint, COUNT(*) AS page_view_count FROM http_logs_prq_15gb \\\n",
    "                        GROUP BY endpoint \\\n",
    "                        ORDER BY page_view_count DESC LIMIT 5\")\n",
    "query2_15gb.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------+\n",
      "|status|distinct_status|\n",
      "+------+---------------+\n",
      "|   404|          83596|\n",
      "|   403|            900|\n",
      "|   500|            260|\n",
      "|   501|            164|\n",
      "|   400|             60|\n",
      "+------+---------------+\n",
      "\n",
      "CPU times: user 2.08 ms, sys: 303 µs, total: 2.39 ms\n",
      "Wall time: 992 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "query3_1gb = spark.sql(\"SELECT status, count(status) AS distinct_status FROM http_logs_prq_1gb \\\n",
    "                        WHERE status >= '400' \\\n",
    "                        GROUP BY status \\\n",
    "                        ORDER BY distinct_status DESC\")\n",
    "query3_1gb.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------+\n",
      "|status|distinct_status|\n",
      "+------+---------------+\n",
      "|   404|         355283|\n",
      "|   403|           3825|\n",
      "|   500|           1105|\n",
      "|   501|            697|\n",
      "|   400|            255|\n",
      "+------+---------------+\n",
      "\n",
      "CPU times: user 2.53 ms, sys: 1 µs, total: 2.53 ms\n",
      "Wall time: 366 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "query3_5gb = spark.sql(\"SELECT status, count(status) AS distinct_status FROM http_logs_prq_5gb \\\n",
    "                        WHERE status >= '400' \\\n",
    "                        GROUP BY status \\\n",
    "                        ORDER BY distinct_status DESC\")\n",
    "query3_5gb.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------+\n",
      "|status|distinct_status|\n",
      "+------+---------------+\n",
      "|   404|         689667|\n",
      "|   403|           7425|\n",
      "|   500|           2145|\n",
      "|   501|           1353|\n",
      "|   400|            495|\n",
      "+------+---------------+\n",
      "\n",
      "CPU times: user 2.93 ms, sys: 0 ns, total: 2.93 ms\n",
      "Wall time: 557 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "query3_10gb = spark.sql(\"SELECT status, count(status) AS distinct_status FROM http_logs_prq_10gb \\\n",
    "                        WHERE status >= '400' \\\n",
    "                        GROUP BY status \\\n",
    "                        ORDER BY distinct_status DESC\")\n",
    "query3_10gb.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------+\n",
      "|status|distinct_status|\n",
      "+------+---------------+\n",
      "|   404|        1044950|\n",
      "|   403|          11250|\n",
      "|   500|           3250|\n",
      "|   501|           2050|\n",
      "|   400|            750|\n",
      "+------+---------------+\n",
      "\n",
      "CPU times: user 2.2 ms, sys: 0 ns, total: 2.2 ms\n",
      "Wall time: 659 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "query3_15gb = spark.sql(\"SELECT status, count(status) AS distinct_status FROM http_logs_prq_15gb \\\n",
    "                        WHERE status >= '400' \\\n",
    "                        GROUP BY status \\\n",
    "                        ORDER BY distinct_status DESC\")\n",
    "query3_15gb.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------+\n",
      "|            endpoint|count_of_requests|\n",
      "+--------------------+-----------------+\n",
      "|/pub/winvn/readme...|             8016|\n",
      "|/pub/winvn/releas...|             6928|\n",
      "|/shuttle/missions...|             2732|\n",
      "|/shuttle/missions...|             1712|\n",
      "|/history/apollo/a...|             1536|\n",
      "+--------------------+-----------------+\n",
      "\n",
      "CPU times: user 3.27 ms, sys: 0 ns, total: 3.27 ms\n",
      "Wall time: 511 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "query4_1gb = spark.sql(\"SELECT endpoint, count(endpoint) AS count_of_requests \\\n",
    "                        FROM http_logs_prq_1gb WHERE status >= '400' \\\n",
    "                        GROUP BY endpoint \\\n",
    "                        ORDER BY count_of_requests DESC \\\n",
    "                        LIMIT 5\")\n",
    "query4_1gb.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------+\n",
      "|            endpoint|count_of_requests|\n",
      "+--------------------+-----------------+\n",
      "|/pub/winvn/readme...|            34068|\n",
      "|/pub/winvn/releas...|            29444|\n",
      "|/shuttle/missions...|            11611|\n",
      "|/shuttle/missions...|             7276|\n",
      "|/history/apollo/a...|             6528|\n",
      "+--------------------+-----------------+\n",
      "\n",
      "CPU times: user 2.4 ms, sys: 0 ns, total: 2.4 ms\n",
      "Wall time: 647 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "query4_5gb = spark.sql(\"SELECT endpoint, count(endpoint) AS count_of_requests \\\n",
    "                        FROM http_logs_prq_5gb WHERE status >= '400' \\\n",
    "                        GROUP BY endpoint \\\n",
    "                        ORDER BY count_of_requests DESC \\\n",
    "                        LIMIT 5\")\n",
    "query4_5gb.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------+\n",
      "|            endpoint|count_of_requests|\n",
      "+--------------------+-----------------+\n",
      "|/pub/winvn/readme...|            66132|\n",
      "|/pub/winvn/releas...|            57156|\n",
      "|/shuttle/missions...|            22539|\n",
      "|/shuttle/missions...|            14124|\n",
      "|/history/apollo/a...|            12672|\n",
      "+--------------------+-----------------+\n",
      "\n",
      "CPU times: user 2.33 ms, sys: 0 ns, total: 2.33 ms\n",
      "Wall time: 1.05 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "query4_10gb = spark.sql(\"SELECT endpoint, count(endpoint) AS count_of_requests \\\n",
    "                        FROM http_logs_prq_10gb WHERE status >= '400' \\\n",
    "                        GROUP BY endpoint \\\n",
    "                        ORDER BY count_of_requests DESC \\\n",
    "                        LIMIT 5\")\n",
    "query4_10gb.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------+\n",
      "|            endpoint|count_of_requests|\n",
      "+--------------------+-----------------+\n",
      "|/pub/winvn/readme...|           100200|\n",
      "|/pub/winvn/releas...|            86600|\n",
      "|/shuttle/missions...|            34150|\n",
      "|/shuttle/missions...|            21400|\n",
      "|/history/apollo/a...|            19200|\n",
      "+--------------------+-----------------+\n",
      "\n",
      "CPU times: user 2.06 ms, sys: 175 µs, total: 2.23 ms\n",
      "Wall time: 1.36 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "query4_15gb = spark.sql(\"SELECT endpoint, count(endpoint) AS count_of_requests \\\n",
    "                        FROM http_logs_prq_15gb WHERE status >= '400' \\\n",
    "                        GROUP BY endpoint \\\n",
    "                        ORDER BY count_of_requests DESC \\\n",
    "                        LIMIT 5\")\n",
    "query4_15gb.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+----------+\n",
      "|            endpoint|          timestamp|SIZE_IN_MB|\n",
      "+--------------------+-------------------+----------+\n",
      "|/shuttle/countdow...|1995-07-07 14:03:32|         7|\n",
      "|/statistics/1995/...|1995-08-15 20:37:24|         3|\n",
      "|/statistics/1995/...|1995-07-07 10:28:56|         3|\n",
      "|/statistics/1995/...|1995-07-06 10:19:00|         3|\n",
      "|/statistics/1995/...| 1995-08-21 8:43:56|         3|\n",
      "|/statistics/1995/...| 1995-07-09 9:22:14|         3|\n",
      "|/mdss/ped/acs/SDP.ps|1995-07-11 17:29:34|         3|\n",
      "|/statistics/1995/...| 1995-07-05 8:57:07|         3|\n",
      "|/statistics/1995/...|1995-08-07 18:28:57|         3|\n",
      "|/statistics/1995/...|1995-08-03 15:51:23|         3|\n",
      "+--------------------+-------------------+----------+\n",
      "\n",
      "CPU times: user 2.9 ms, sys: 480 µs, total: 3.38 ms\n",
      "Wall time: 10.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "query5_1gb = spark.sql(\"SELECT DISTINCT(endpoint), timestamp, ROUND((object_size * 0.000001)) AS SIZE_IN_MB \\\n",
    "                        FROM http_logs_prq_1gb \\\n",
    "                        ORDER BY SIZE_IN_MB DESC \\\n",
    "                        LIMIT 10\")\n",
    "query5_1gb.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+----------+\n",
      "|            endpoint|          timestamp|SIZE_IN_MB|\n",
      "+--------------------+-------------------+----------+\n",
      "|/shuttle/countdow...|1995-07-07 14:03:32|         7|\n",
      "|/statistics/1995/...|1995-08-15 20:37:24|         3|\n",
      "|/statistics/1995/...|1995-07-07 10:28:56|         3|\n",
      "|/statistics/1995/...|1995-07-06 10:19:00|         3|\n",
      "|/statistics/1995/...| 1995-08-21 8:43:56|         3|\n",
      "|/statistics/1995/...| 1995-07-09 9:22:14|         3|\n",
      "|/mdss/ped/acs/SDP.ps|1995-07-11 17:29:34|         3|\n",
      "|/statistics/1995/...| 1995-07-05 8:57:07|         3|\n",
      "|/statistics/1995/...|1995-08-07 18:28:57|         3|\n",
      "|/statistics/1995/...|1995-08-03 15:51:23|         3|\n",
      "+--------------------+-------------------+----------+\n",
      "\n",
      "CPU times: user 0 ns, sys: 4.29 ms, total: 4.29 ms\n",
      "Wall time: 12.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "query5_5gb = spark.sql(\"SELECT DISTINCT(endpoint), timestamp, ROUND((object_size * 0.000001)) AS SIZE_IN_MB \\\n",
    "                        FROM http_logs_prq_5gb \\\n",
    "                        ORDER BY SIZE_IN_MB DESC \\\n",
    "                        LIMIT 10\")\n",
    "query5_5gb.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+----------+\n",
      "|            endpoint|          timestamp|SIZE_IN_MB|\n",
      "+--------------------+-------------------+----------+\n",
      "|/shuttle/countdow...|1995-07-07 14:03:32|         7|\n",
      "|/statistics/1995/...|1995-08-15 20:37:24|         3|\n",
      "|/statistics/1995/...|1995-07-07 10:28:56|         3|\n",
      "|/statistics/1995/...|1995-07-06 10:19:00|         3|\n",
      "|/statistics/1995/...| 1995-08-21 8:43:56|         3|\n",
      "|/statistics/1995/...| 1995-07-09 9:22:14|         3|\n",
      "|/mdss/ped/acs/SDP.ps|1995-07-11 17:29:34|         3|\n",
      "|/statistics/1995/...| 1995-07-05 8:57:07|         3|\n",
      "|/statistics/1995/...|1995-08-07 18:28:57|         3|\n",
      "|/statistics/1995/...|1995-08-03 15:51:23|         3|\n",
      "+--------------------+-------------------+----------+\n",
      "\n",
      "CPU times: user 4.28 ms, sys: 362 µs, total: 4.64 ms\n",
      "Wall time: 23.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "query5_10gb = spark.sql(\"SELECT DISTINCT(endpoint), timestamp, ROUND((object_size * 0.000001)) AS SIZE_IN_MB \\\n",
    "                        FROM http_logs_prq_10gb \\\n",
    "                        ORDER BY SIZE_IN_MB DESC \\\n",
    "                        LIMIT 10\")\n",
    "query5_10gb.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+----------+\n",
      "|            endpoint|          timestamp|SIZE_IN_MB|\n",
      "+--------------------+-------------------+----------+\n",
      "|/shuttle/countdow...|1995-07-07 14:03:32|         7|\n",
      "|/statistics/1995/...|1995-08-15 20:37:24|         3|\n",
      "|/statistics/1995/...|1995-07-07 10:28:56|         3|\n",
      "|/statistics/1995/...|1995-07-06 10:19:00|         3|\n",
      "|/statistics/1995/...| 1995-08-21 8:43:56|         3|\n",
      "|/statistics/1995/...| 1995-07-09 9:22:14|         3|\n",
      "|/mdss/ped/acs/SDP.ps|1995-07-11 17:29:34|         3|\n",
      "|/statistics/1995/...| 1995-07-05 8:57:07|         3|\n",
      "|/statistics/1995/...|1995-08-07 18:28:57|         3|\n",
      "|/statistics/1995/...|1995-08-03 15:51:23|         3|\n",
      "+--------------------+-------------------+----------+\n",
      "\n",
      "CPU times: user 3.98 ms, sys: 413 µs, total: 4.39 ms\n",
      "Wall time: 10.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "query5_15gb = spark.sql(\"SELECT DISTINCT(endpoint), timestamp, ROUND((object_size * 0.000001)) AS SIZE_IN_MB \\\n",
    "                        FROM http_logs_prq_5gb \\\n",
    "                        ORDER BY SIZE_IN_MB DESC \\\n",
    "                        LIMIT 10\")\n",
    "query5_15gb.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1gb.write.orc(\"./ORC-Files/nasa_logs_1GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
